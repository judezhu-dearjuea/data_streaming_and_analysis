{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Understanding User Behavior\n",
    "**Project Team: Jude Wentian Zhu, Rohit Barkshi, Rathin Bector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Project Files\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Pipeline\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "\n",
    "### Step 1: Spin up Docker-Compose and Link Pypark to Jupyter Notebook\n",
    "\n",
    "1. Spin up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"project_3_default\" with the default driver\n",
      "Creating project_3_presto_1 ... \n",
      "Creating project_3_redis_1  ... \n",
      "Creating project_3_cloudera_1 ... \n",
      "Creating project_3_zookeeper_1 ... \n",
      "Creating project_3_mids_1      ... \n",
      "\u001b[3Bting project_3_cloudera_1  ... \u001b[32mdone\u001b[0m\u001b[3A\u001b[2KCreating project_3_spark_1     ... \n",
      "\u001b[3BCreating project_3_kafka_1     ... mdone\u001b[0m\u001b[3A\u001b[2K\n",
      "\u001b[1Bting project_3_kafka_1     ... \u001b[32mdone\u001b[0m\u001b[2A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exec a bash shell in the spark container on teminal.\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "\n",
    "\n",
    "3. Create a symbolic link from the spark directory to /w205 :\n",
    "```console\n",
    "ln -s /w205 w205\n",
    "```\n",
    "\n",
    "\n",
    "4. Exit the container\n",
    "```console\n",
    "exit\n",
    "```\n",
    "\n",
    "\n",
    "5. Check out Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - hive   supergroup          0 2021-08-05 04:36 /tmp/hive\n",
      "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Launch Kafka and Flask\n",
    "\n",
    "1. Create a kafka topic called events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic events.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install dependencies for flask app on mids container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting redis\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |################################| 81kB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: redis\n",
      "Successfully installed redis-3.5.3\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting numpy==1.14.6\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/5b/1077ec0ebfa06f42057e8315bc8e05f5978b6fd0f582879f35f4d62ff124/numpy-1.14.6-cp27-cp27mu-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |################################| 13.8MB 90kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.14.6\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids pip install redis\n",
    "!docker-compose exec mids pip install numpy==1.14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run flask app on teminal\n",
    "```console\n",
    "docker-compose exec mids env FLASK_APP=/w205/project_3/game_api.py flask run --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!bash data_generator.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. read events from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: docker-compose: not found\r\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning -e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defin schema\n",
    "def player_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def purchase_weapon_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- weapon: string (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"weapon\", StringType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def purchase_shield_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def dig_for_gold_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- gold_found: integer (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"gold_found\", IntegerType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def successful_attack_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- attacker: string (nullable = true)\n",
    "    |-- defender: string (nullable = true)\n",
    "    |-- weapon_used: string (nullable = true)\n",
    "    |-- defender_has_shield: boolean (nullable = true)\n",
    "    |-- defender_health_before:integer  (nullable = true)\n",
    "    |-- defender_health_after:integer  (nullable = true)\n",
    "    |-- defender_killed: boolean (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"attacker\", StringType(), True),\n",
    "        StructField(\"defender\", StringType(), True),\n",
    "        StructField(\"weapon_used\", StringType(), True),\n",
    "        StructField(\"defender_has_shield\", BooleanType(), True),\n",
    "        StructField(\"defender_health_before\", IntegerType(), True),\n",
    "        StructField(\"defender_health_after\", IntegerType(), True),\n",
    "        StructField(\"defender_killed\", BooleanType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def failed_attack_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- attacker: string (nullable = true)\n",
    "    |-- defender: string (nullable = true)\n",
    "    |-- weapon_used: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"attacker\", StringType(), True),\n",
    "        StructField(\"defender\", StringType(), True),\n",
    "        StructField(\"weapon_used\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define functions\n",
    "@udf('boolean')\n",
    "def is_player(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'initialize_player':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_weapon(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    # m = re.match('purchase',event['event_type'])\n",
    "    if event['event_type'] == 'purchase_weapon':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_shield(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_dig_for_gold(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'dig_for_gold':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_shield(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_successful_attack(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'successful_attack':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_failed_attack(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'failed_attack':\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start reading events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ExtractEventsJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_player = raw_events \\\n",
    "    .filter(is_player(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      player_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "dig_for_gold = raw_events \\\n",
    "    .filter(is_dig_for_gold(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "        raw_events.timestamp.cast('string'),\n",
    "        from_json(raw_events.value.cast('string'),\n",
    "                  dig_for_gold_event_schema()).alias('json')) \\\n",
    ".select('raw_event', 'timestamp', 'json.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-----------------+---------------+-----------------+--------+---------+\n",
      "|           raw_event|           timestamp|Accept|             Host|     User-Agent|       event_type|username|timestamp|\n",
      "+--------------------+--------------------+------+-----------------+---------------+-----------------+--------+---------+\n",
      "|{\"username\": \"use...|2021-08-05 20:07:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user50|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:07:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user29|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:53:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user36|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:53:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user41|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:54:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user20|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user43|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user13|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user18|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user10|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user34|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user28|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user12|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user46|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user22|     null|\n",
      "|{\"username\": \"use...|2021-08-05 20:57:...|   */*|user1.comcast.com|ApacheBench/2.3|initialize_player|  user24|     null|\n",
      "+--------------------+--------------------+------+-----------------+---------------+-----------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initialize_player.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-----------------+---------------+------------+--------+----------+-------------+------------+---------+\n",
      "|           raw_event|           timestamp|Accept|             Host|     User-Agent|  event_type|username|gold_found|wallet_before|wallet_after|timestamp|\n",
      "+--------------------+--------------------+------+-----------------+---------------+------------+--------+----------+-------------+------------+---------+\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        19|          100|         114|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|         3|          114|         112|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        43|          112|         150|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        16|          150|         161|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        16|          161|         172|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        12|          172|         179|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|         6|          179|         180|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|         9|          180|         184|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        31|          184|         210|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:26:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        36|          210|         241|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user6|        12|           25|          32|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user6|        33|           32|          60|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user6|        35|           60|          90|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user6|        17|           90|         102|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user6|         0|          102|          97|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        14|          191|         200|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        24|          200|         219|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        75|          219|         289|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        14|          289|         298|     null|\n",
      "|{\"username\": \"use...|2021-08-05 18:27:...|   */*|user1.comcast.com|ApacheBench/2.3|dig_for_gold|   user3|        19|          298|         312|     null|\n",
      "+--------------------+--------------------+------+-----------------+---------------+------------+--------+----------+-------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dig_for_gold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ExtractEventsJob\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract initial_player events\n",
    "extracted_initialize_player = raw_events \\\n",
    "    .filter(is_player(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      player_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink_player = extracted_initialize_player \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/player\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_player\") \\\n",
    "    .start()\n",
    "\n",
    "#extract dig for gold events\n",
    "extracted_dig_for_gold = raw_events \\\n",
    "    .filter(is_dig_for_gold(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                  dig_for_gold_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_dig_for_gold = extracted_dig_for_gold \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/dig_for_gold\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_dig_for_gold\") \\\n",
    "    .start()\n",
    "    \n",
    "    \n",
    "#extract purachase_weapon events\n",
    "extracted_purchase_weapon = raw_events \\\n",
    "    .filter(is_purchase_weapon(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_weapon_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_purchase_weapon = extracted_purchase_weapon \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/purchase_weapon\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_purchase_weapon\") \\\n",
    "    .start()\n",
    "\n",
    "    \n",
    "#extract purachase_shield events\n",
    "extracted_purchase_shield = raw_events \\\n",
    "    .filter(is_purchase_shield(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_shield_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "            \n",
    "sink_purchase_shield = extracted_purchase_shield \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/purchase_shield\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_purchase_shield\") \\\n",
    "    .start()\n",
    "            \n",
    "#extract successful_attack events\n",
    "extracted_successful_attack = raw_events \\\n",
    "    .filter(is_successful_attack(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      successful_attack_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_successful_attack= extracted_successful_attack \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/successful_attack\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_successful_attack\") \\\n",
    "    .start()\n",
    "            \n",
    "            \n",
    "#extract failed_attack events\n",
    "extracted_failed_attack = raw_events \\\n",
    "    .filter(is_failed_attack(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      failed_attack_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_failed_attack= extracted_failed_attack \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/failed_attack\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_failed_attack\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink_player.stop()\n",
    "sink_purchase_weapon.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'timestamp' is ambiguous, could be: timestamp#4415, timestamp#4421.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1061.collectToPython.\n: org.apache.spark.sql.AnalysisException: Reference 'timestamp' is ambiguous, could be: timestamp#4415, timestamp#4421.;\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:287)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:181)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:153)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:152)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:98)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat org.apache.spark.sql.types.StructType.map(StructType.scala:98)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:152)\n\tat org.apache.spark.sql.execution.datasources.FileSourceStrategy$.apply(FileSourceStrategy.scala:83)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:63)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2823)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2800)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-dcf23f6c13d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtransformed_player_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'player'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformed_player_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtransformed_player_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \"\"\"\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'timestamp' is ambiguous, could be: timestamp#4415, timestamp#4421.;\""
     ]
    }
   ],
   "source": [
    "transformed_player_events = spark.read.parquet('/tmp/player')\n",
    "transformed_player_events.registerTempTable('player')\n",
    "\n",
    "transformed_player_events.toPandas()\n",
    "transformed_player_events.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use Spark to Batch Analysis"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

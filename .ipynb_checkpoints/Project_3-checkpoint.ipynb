{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb58e129",
   "metadata": {},
   "source": [
    "# Project 3: Understanding User Behavior\n",
    "**Project Team: Jude Wentian Zhu, Rohit Barkshi, Rathin Bector**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b517f",
   "metadata": {},
   "source": [
    "## Description of Project Files\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db2653",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c9341",
   "metadata": {},
   "source": [
    "## Summary of Data Pipeline\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f6739",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "\n",
    "### Step 1: Spin up Docker-Compose and Link Pypark to Jupyter Notebook\n",
    "\n",
    "1. Spin up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522373f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"project_3_default\" with the default driver\n",
      "Creating project_3_zookeeper_1 ... \n",
      "Creating project_3_mids_1      ... \n",
      "Creating project_3_presto_1    ... \n",
      "Creating project_3_cloudera_1  ... \n",
      "\u001b[4BCreating project_3_kafka_1     ... mdone\u001b[0m\n",
      "\u001b[2BCreating project_3_spark_1     ... mdone\u001b[0m\n",
      "\u001b[1Bting project_3_spark_1     ... \u001b[32mdone\u001b[0m\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c7bce",
   "metadata": {},
   "source": [
    "2. Exec a bash shell in the spark container on teminal.\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "\n",
    "\n",
    "3. Create a symbolic link from the spark directory to /w205 :\n",
    "```console\n",
    "ln -s /w205 w205\n",
    "```\n",
    "\n",
    "\n",
    "4. Exit the container\n",
    "```console\n",
    "exit\n",
    "```\n",
    "\n",
    "\n",
    "5. Check out Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534b09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - hive   supergroup          0 2021-08-01 19:22 /tmp/hive\n",
      "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b33aec",
   "metadata": {},
   "source": [
    "### Step 2: Launch Kafka and Flask\n",
    "\n",
    "1. Create a kafka topic called events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9c11d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic events.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007bc96",
   "metadata": {},
   "source": [
    "2. Run flask app on teminal\n",
    "```console\n",
    "docker-compose exec mids env FLASK_APP=/w205/project_3/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "\n",
    "3. Read events from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24265d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Reached end of topic events [0] at offset 0: exiting\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning -e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8ddba",
   "metadata": {},
   "source": [
    "### Step 3: Use Spark to Batch Process Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a8302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5c141",
   "metadata": {},
   "source": [
    "### Step 4: Use Spark to Stream Events"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

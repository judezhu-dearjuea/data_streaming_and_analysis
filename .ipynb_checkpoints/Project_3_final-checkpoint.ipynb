{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Understanding User Behavior\n",
    "**Project Team: Jude Wentian Zhu, Rohit Barkshi, Rathin Bector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Project Files\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Pipeline\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Explanation of Game (game_api.py)\n",
    "\n",
    "Our game is a simple multiplayer fighting game. Each player initializes with a set amount of health, money, and a basic weapon (stick). They can purchase another weapon with their money or purchase a shield to limit their damage intake. To get more money, they can \"dig for gold\". A player attacks other players with their weapon with the hope of killing the other players (bringing their health to 0). If they are successful in killing another player, they inherit all of the other player's money.\n",
    "\n",
    "### Explanation of function calls:\n",
    "- /initialize\n",
    "    - Person calls this to initialize a new player in the game with a given username. The player is initialized with 100 money, 20 health, a stick (weapon) and no shield.\n",
    "    - The event is logged to Kafka.\n",
    "- /purchase_weapon\n",
    "    - Person calls this to purchase a weapon in the game with their given username and desired weapon. There are checks for validity of username and weapon, aliveness of player, and sufficiency of money. If purchase is successful, the appropriate money is subtracted from the wallet, and the new weapon is stored for the player.\n",
    "    - Types of Weapon: Price, Damage, Success Rate of Weapon\n",
    "        - Stick: 0, 0, 1\n",
    "        - Knife: 5, 2, 0.7\n",
    "        - Sword: 10, 3, 0.65\n",
    "        - Grenade: 15, 5, 0.4\n",
    "        - Gun: 50, 9, 0.75\n",
    "        - Bazooka: 100, 20, 0.5\n",
    "        - Nuke: 500, 100, 1\n",
    "    - A successful purchase is logged to Kafka.\n",
    "- /purchase_shield\n",
    "    - Person calls this to purchase a shield in the game. There are checks for validity of username, aliveness of player, and sufficiency of money. If purchase is successful, the appropriate money is subtracted from the wallet, and the player is equipped with a shield.\n",
    "     - Shield costs 20 and limits the damage of attacks by 5.\n",
    "     - A successful purchase is logged to Kafka.\n",
    "- /dig_for_gold\n",
    "    - Person calls this to \"dig for gold\" or get more money. There are checks for validity of username, aliveness of player, and sufficiency of money to purchase a shovel to \"dig for gold\". If purchase is successful, the player digs for gold. A random distribution determines the amount of gold the player will receive, and this amount is added to the player's wallet. Digging for gold costs 5.\n",
    "     - A successful \"dig for gold\" event is logged to Kafka.\n",
    "- /attack\n",
    "    - Person calls this to attack another player. There are checks for validity of both usernames and aliveness of players. The attack has a chance of success based on the player's weapon. If the attack is successful, the enemy player's health is reduced by the player's weapon's damage rate. If the enemy has a shield, the damage rate is reduced by 5 health. The enemy player is killed if their health reaches 0. If the enemy is killed, the player inherits all of the enemy's money in their wallet.\n",
    "    - A successful attack is logged to Kafka.\n",
    "    - A failed attack is logged to Kafka.\n",
    "\n",
    "### Bonus (Redis)\n",
    "We use redis to track the player state and store their data in the game. We use redis as a distributed, in memory key-value storage engine and populate it with different types of keys and values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "\n",
    "### Step 1: Spin up Docker-Compose and Link Pypark to Jupyter Notebook\n",
    "\n",
    "1. Spin up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"project_3_default\" with the default driver\n",
      "Creating project_3_presto_1 ... \n",
      "Creating project_3_redis_1  ... \n",
      "Creating project_3_cloudera_1 ... \n",
      "Creating project_3_zookeeper_1 ... \n",
      "Creating project_3_mids_1      ... \n",
      "\u001b[3Bting project_3_cloudera_1  ... \u001b[32mdone\u001b[0m\u001b[3A\u001b[2KCreating project_3_spark_1     ... \n",
      "\u001b[3BCreating project_3_kafka_1     ... mdone\u001b[0m\u001b[3A\u001b[2K\n",
      "\u001b[1Bting project_3_kafka_1     ... \u001b[32mdone\u001b[0m\u001b[2A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exec a bash shell in the spark container on teminal.\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "\n",
    "\n",
    "3. Create a symbolic link from the spark directory to /w205 :\n",
    "```console\n",
    "ln -s /w205 w205\n",
    "```\n",
    "\n",
    "\n",
    "4. Exit the container\n",
    "```console\n",
    "exit\n",
    "```\n",
    "\n",
    "\n",
    "5. Check out Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - hive   supergroup          0 2021-08-05 04:36 /tmp/hive\n",
      "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Launch Kafka and Flask\n",
    "\n",
    "1 Create a kafka topic called events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic events.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Install dependencies for flask app on mids container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting redis\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |################################| 81kB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: redis\n",
      "Successfully installed redis-3.5.3\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting numpy==1.14.6\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/5b/1077ec0ebfa06f42057e8315bc8e05f5978b6fd0f582879f35f4d62ff124/numpy-1.14.6-cp27-cp27mu-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |################################| 13.8MB 90kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.14.6\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids pip install redis\n",
    "!docker-compose exec mids pip install numpy==1.14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bring up game_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!docker-compose exec mids env FLASK_APP=/w205/project_3/game_api.py flask run --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start a Jupyter notebook for a pyspark kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Streaming Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** define the Schema **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defin schema\n",
    "def player_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def purchase_weapon_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- weapon: string (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"weapon\", StringType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "def purchase_shield_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "def dig_for_gold_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- username: string (nullable = true)\n",
    "    |-- gold_found: integer (nullable = true)\n",
    "    |-- wallet_before:integer  (nullable = true)\n",
    "    |-- wallet_after:integer  (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"gold_found\", IntegerType(), True),\n",
    "        StructField(\"wallet_before\", IntegerType(), True),\n",
    "        StructField(\"wallet_after\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "def successful_attack_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- attacker: string (nullable = true)\n",
    "    |-- defender: string (nullable = true)\n",
    "    |-- weapon_used: string (nullable = true)\n",
    "    |-- defender_has_shield: boolean (nullable = true)\n",
    "    |-- defender_health_before:integer  (nullable = true)\n",
    "    |-- defender_health_after:integer  (nullable = true)\n",
    "    |-- defender_killed: boolean (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"attacker\", StringType(), True),\n",
    "        StructField(\"defender\", StringType(), True),\n",
    "        StructField(\"weapon_used\", StringType(), True),\n",
    "        StructField(\"defender_has_shield\", BooleanType(), True),\n",
    "        StructField(\"defender_health_before\", IntegerType(), True),\n",
    "        StructField(\"defender_health_after\", IntegerType(), True),\n",
    "        StructField(\"defender_killed\", BooleanType(), True),\n",
    "    ])\n",
    "\n",
    "def failed_attack_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- attacker: string (nullable = true)\n",
    "    |-- defender: string (nullable = true)\n",
    "    |-- weapon_used: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"attacker\", StringType(), True),\n",
    "        StructField(\"defender\", StringType(), True),\n",
    "        StructField(\"weapon_used\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define functions\n",
    "@udf('boolean')\n",
    "def is_player(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'initialize_player':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_weapon(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    # m = re.match('purchase',event['event_type'])\n",
    "    if event['event_type'] == 'purchase_weapon':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_shield(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_dig_for_gold(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'dig_for_gold':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase_shield(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_successful_attack(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'successful_attack':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_failed_attack(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'failed_attack':\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start reading events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ExtractEventsJob\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Start Writing Stream **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract initial_player events\n",
    "extracted_initialize_player = raw_events \\\n",
    "    .filter(is_player(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      player_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink_player = extracted_initialize_player \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/player\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_player\") \\\n",
    "    .start()\n",
    "    \n",
    "    \n",
    "#extract purachase_weapon events\n",
    "extracted_purchase_weapon = raw_events \\\n",
    "    .filter(is_purchase_weapon(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_weapon_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_purchase_weapon = extracted_purchase_weapon \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/purchase_weapon\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_purchase_weapon\") \\\n",
    "    .start()\n",
    "\n",
    "    \n",
    "#extract purachase_shield events\n",
    "extracted_purchase_shield = raw_events \\\n",
    "    .filter(is_purchase_shield(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_shield_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "            \n",
    "sink_purchase_shield = extracted_purchase_shield \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/purchase_shield\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_purchase_shield\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "#extract dig for gold events\n",
    "extracted_dig_for_gold = raw_events \\\n",
    "    .filter(is_dig_for_gold(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                  dig_for_gold_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_dig_for_gold = extracted_dig_for_gold \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/dig_for_gold\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_dig_for_gold\") \\\n",
    "    .start()    \n",
    "\n",
    "#extract successful_attack events\n",
    "extracted_successful_attack = raw_events \\\n",
    "    .filter(is_successful_attack(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      successful_attack_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_successful_attack= extracted_successful_attack \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/successful_attack\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_successful_attack\") \\\n",
    "    .start()\n",
    "            \n",
    "            \n",
    "#extract failed_attack events\n",
    "extracted_failed_attack = raw_events \\\n",
    "    .filter(is_failed_attack(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      failed_attack_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "sink_failed_attack= extracted_failed_attack \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",\"/tmp/failed_attack\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_failed_attack\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Start generating events using bash script, the loop runs 1000 times\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 27: docker-compose: command not found\n",
      "data_generator.sh: line 28: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 46: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 33: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 43: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 37: docker-compose: command not found\n",
      "data_generator.sh: line 40: docker-compose: command not found\n"
     ]
    }
   ],
   "source": [
    "!bash data_generator.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** stop writing stream **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sink_player.stop()\n",
    "sink_purchase_weapon.stop()\n",
    "sink_purchase_shield.stop()\n",
    "sink_dig_for_gold.stop()\n",
    "sink_successful_attack.stop()\n",
    "sink_failed_attack.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check Kafka **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform events, write to hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'`default`.`player_events` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.sql.\n: org.apache.spark.sql.AnalysisException: `default`.`player_events` already exists.;\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:52)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-51559be614fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplayer_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"create external table player_events stored as parquet location '/tmp/player_events' as select * from player\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: '`default`.`player_events` already exists.;'"
     ]
    }
   ],
   "source": [
    "#player_events\n",
    "transformed_player_events = spark.read.parquet('/tmp/player')\n",
    "transformed_player_events.registerTempTable('player')\n",
    "\n",
    "player_query = \"create external table player_events stored as parquet location '/tmp/player_events' as select * from player\"\n",
    "spark.sql(player_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#purchase_weapon_events\n",
    "transformed_purchase_weapon_events = spark.read.parquet('/tmp/purchase_weapon')\n",
    "transformed_purchase_weapon_events.registerTempTable('purchase_weapon')\n",
    "\n",
    "purchase_weapon_query = \"create external table purchase_weapon_events stored as parquet location '/tmp/purchase_weapon_events' as select * from purchase_weapon\"\n",
    "spark.sql(purchase_weapon_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#purchase_shield_events\n",
    "transformed_purchase_shield_events = spark.read.parquet('/tmp/purchase_shield')\n",
    "transformed_purchase_shield_events.registerTempTable('purchase_shield')\n",
    "\n",
    "purchase_shield_query = \"create external table purchase_shield_events stored as parquet location '/tmp/purchase_shield_events' as select * from purchase_shield\"\n",
    "spark.sql(purchase_shield_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dig_for_gold_events\n",
    "transformed_dig_for_gold_events = spark.read.parquet('/tmp/dig_for_gold')\n",
    "transformed_dig_for_gold_events.registerTempTable('dig_for_gold')\n",
    "\n",
    "dig_for_gold_query = \"create external table dig_for_gold_events stored as parquet location '/tmp/dig_for_gold_events' as select * from dig_for_gold\"\n",
    "spark.sql(dig_for_gold_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#successful_attack_events\n",
    "transformed_successful_attack_events = spark.read.parquet('/tmp/successful_attack')\n",
    "transformed_successful_attack_events.registerTempTable('successful_attack')\n",
    "\n",
    "successful_attack_query = \"create external table successful_attack_events stored as parquet location '/tmp/successful_attack_events' as select * from successful_attack\"\n",
    "spark.sql(successful_attack_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#failed_attack_events\n",
    "transformed_failed_attack_events = spark.read.parquet('/tmp/failed_attack')\n",
    "transformed_failed_attack_events.registerTempTable('failed_attack')\n",
    "\n",
    "failed_attack_query = \"create external table failed_attack_events stored as parquet location '/tmp/failed_attack_events' as select * from failed_attack\"\n",
    "spark.sql(failed_attack_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** using pandas to check that transformed events **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of Empty DataFrame\n",
       "Columns: [raw_event, timestamp, Accept, Host, User-Agent, event_type, attacker, defender, weapon_used, defender_has_shield, defender_health_before, defender_health_after, defender_killed]\n",
       "Index: []>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_successful_attack_pd = transformed_successful_attack_events.toPandas()\n",
    "transformed_successful_attack_pd.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** another check **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of Empty DataFrame\n",
       "Columns: [raw_event, timestamp, Accept, Host, User-Agent, event_type, attacker, defender, weapon_used]\n",
       "Index: []>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed__pd = transformed_failed_attack_events.toPandas()\n",
    "transformed_failed_attack_pd.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis using Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1 run Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker-compose exec presto presto --server presto:8080 --catalog hive --schema default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 check and describe tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the tables in presto\n",
    "!show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#describe the tables\n",
    "!describe player_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 start business analysis in Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q1: How many distinct players have been created **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT COUNT(DISTINCT(username)) FROM player_events;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 distinct players have been created (actually there won't be any duplicates on username because our game does not allow repeated username to be created using Redis state tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q2 How often is each weapon purchased?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT weapon, COUNT(weapon) as weapon_purchase_count FROM purchase_weapon_events GROUP BY weapon OR\n",
    "DER BY weapon_purchase_count DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q3 What is the average wallet size after the purchase of a shield? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "???!SELECT AVG(wallet_after) FROM purchase_shield_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q4 How many players purchased a shield more than once?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT COUNT(shield_purchases) FROM (SELECT username, COUNT(username) AS shield_purchases FROM purchase_shield_events GROUP BY(username));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q5 What is the max, min, and average of gold found by a user?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT MAX(gold_found) AS max, MIN(gold_found) AS min, AVG(gold_found) AS avg FROM dig_for_gold_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q6 What weapon fails the most often? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT weapon_used, COUNT(weapon_used) as count_weapon_used FROM failed_attack_events GROUP BY weapon_used;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q7 Which weapon kills the most often? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT weapon_used, COUNT(weapon_used) AS count_weapon_used FROM successful_attack_events where defender_killed = true GROUP BY weapon_used;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q8 How many players have been killed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT count(defender_killed) as count_defender_killed FROM successful_attack_events WHERE defender\n",
    "_killed = true;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q9 How many defenders has a shield?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!SELECT count(defender) AS count_has_shield FROM successful_attack_events WHERE defender_has_shield \n",
    "= true;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bring down the docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker-compose down"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

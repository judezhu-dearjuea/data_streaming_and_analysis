{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Understanding User Behavior\n",
    "**Project Team: Jude Wentian Zhu, Rohit Barkshi, Rathin Bector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Project Files\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Pipeline\n",
    "\n",
    "<span style=\"color:red\">**Need to fill in**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "\n",
    "### Step 1: Spin up Docker-Compose and Link Pypark to Jupyter Notebook\n",
    "\n",
    "1. Spin up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"project_3_default\" with the default driver\n",
      "Creating project_3_presto_1 ... \n",
      "Creating project_3_redis_1  ... \n",
      "Creating project_3_cloudera_1 ... \n",
      "Creating project_3_zookeeper_1 ... \n",
      "Creating project_3_mids_1      ... \n",
      "\u001b[3Bting project_3_cloudera_1  ... \u001b[32mdone\u001b[0m\u001b[3A\u001b[2KCreating project_3_spark_1     ... \n",
      "\u001b[3BCreating project_3_kafka_1     ... mdone\u001b[0m\u001b[3A\u001b[2K\n",
      "\u001b[1Bting project_3_kafka_1     ... \u001b[32mdone\u001b[0m\u001b[2A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exec a bash shell in the spark container on teminal.\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "\n",
    "\n",
    "3. Create a symbolic link from the spark directory to /w205 :\n",
    "```console\n",
    "ln -s /w205 w205\n",
    "```\n",
    "\n",
    "\n",
    "4. Exit the container\n",
    "```console\n",
    "exit\n",
    "```\n",
    "\n",
    "\n",
    "5. Check out Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - hive   supergroup          0 2021-08-05 04:36 /tmp/hive\n",
      "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Launch Kafka and Flask\n",
    "\n",
    "1. Create a kafka topic called events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic events.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install dependencies for flask app on mids container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting redis\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |################################| 81kB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: redis\n",
      "Successfully installed redis-3.5.3\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting numpy==1.14.6\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/5b/1077ec0ebfa06f42057e8315bc8e05f5978b6fd0f582879f35f4d62ff124/numpy-1.14.6-cp27-cp27mu-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |################################| 13.8MB 90kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.14.6\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids pip install redis\n",
    "!docker-compose exec mids pip install numpy==1.14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run flask app on teminal\n",
    "```console\n",
    "docker-compose exec mids env FLASK_APP=/w205/project_3/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "\n",
    "4. Read events from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Reached end of topic events [0] at offset 0: exiting\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning -e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use Spark to Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_purchase_weapon(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    # m = re.match('purchase',event['event_type'])\n",
    "    if event['event_type'] == 'purchase_weapon':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_purchase_shield(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    # m = re.match('purchase',event['event_type'])\n",
    "    if event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ExtractEventsJob\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchase_weapon = raw_events \\\n",
    "    .filter(is_purchase_weapon(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_sword_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of DataFrame[raw: string, timestamp: string]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
